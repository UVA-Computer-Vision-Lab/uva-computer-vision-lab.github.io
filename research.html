

<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <title>Computer Vision at University of Virginia</title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="description" content="Computer Vision Lab at the University of Virginia">
      <meta name="author" content="">
      <!-- Le styles -->
      <link href="css/bootstrap.min.css" rel="stylesheet">
      <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
      <link href="css/theme.css" rel="stylesheet">
      <!-- Under Construction Starts -->
      <style>
         .under-construction {
            background-color: red;
            color: white;
            text-align: center;
            padding: 20px;
            font-size: 24px;
            font-weight: bold;
            position: fixed;
            width: 100%;
            top: 0;
            z-index: 1000;
         }
         body {
            padding-top: 0px; /* Adjust for the fixed banner */
         }
         </style>
      <!-- Under Construction Ends -->
   </head>
   <body>
      <div class="container">
         <header class="jumbotron subhead" id="overview">
            <h1>Computer Vision Lab</h1>
            <p class="lead"> University of Virginia </p>
         </header>
         <div class="masthead">
            <div class="navbar">
               <div class="navbar-inner">
                  <div class="container">
                     <ul class="nav">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="people.html">People</a></li>
                        <li class="active"><a href="#">Research</a></li>
                        <li><a href="publications.html">Publications</a></li>
                        <li><a href="teaching.html">Teaching</a></li>
                        <li><a href="fun.html">Lab Fun</a></li>
                        <li><a href="contact.html">Join Us</a></li>
                     </ul>
                  </div>
               </div>
            </div>
         </div>


         <div class="row-fluid">
           
            <div class="span2 bs-docs-sidebar" id="navparent">
               <ul class="nav nav-list bs-docs-sidenav" data-spy="affix" data-offset-top="200" data-offset-bottom="260">
                  <li><a href="#"> Research Focus </a></li>
                  <li><a class="subhead" href="#selfsup"> Label-efficient Learning </a></li>
                  <li><a class="subhead" href="#3d"> Recon. and Recog. in 3D  </a></li>
                  <li><a class="subhead" href="#cross"> Computer Vision + X </a></li>
               </ul>
            </div>
            <div class="span9 offset1">

               <section id="selfsup">
                  <div class="page-header">
                     <h3>Label-efficient Learning</h3>
                    <img  src="images/tasks.png" alt="Vision tasks"/>
                     <p> The cost of collecting human annotations is a significant barrier in many vision tasks.
                     For example, annotating the landmarks or semantic parts of an object is much more time-consuming than categorizing the image; annotating the 3D pose of an object is often done by reasoning with 3D model's projection to the 2D image; annotating objects with fine-grained labels (e.g. Grasshopper sparrow vs. Lincoln's sparrow) requires strong domain-specific expertise. In addition, labeling without clearly defined protocols leads to a variation in labeling styles of different annotators, which can make subsequent learning harder. </p>
                    <img  src="images/label-efficient-website.png" alt="Vision tasks"/>
                    <p> To minimize the annotation cost, one of our research goals is to develop learning algorithms in the context of different vision tasks to reduce the cost of supervision and allow learning from different labeling styles.</p>
                    <p> <b>[Ongoing projects]</b> Our recent efforts in this direction include developing various label-efficient training algorithms in the context of modern vision tasks, such as vision-language models and open-vocabulary 3D recognition and reconstruction.</p>
                  </div>
               </section>

               <section id="3d">
                  <div class="page-header">
                     <h3>Reconstruct and Recognize Anything in 3D</h3>
                    <img  src="images/3d.png" alt="3D"/>
                    <p>
                    Understanding object pose and its 3D structure is a central computer vision problem. 
                    Structure-from-motion (SfM) algorithms are the default solution for this task. 
                    However, SfM degrades in performance significantly on sparse 2D observations; 
                    SfM, along with multi-view stereo (MVS), only provides a coarse description of the 3D geometry and texture;
                    Manipulating 3D shapes requires tremendous human efforts and expertise in 3D software;
                    Recent breakthroughs in learning neural scene representations (e.g. NeRF) have shown that the long-standing problem of photo-realistic novel view synthesis and detailed 3D reconstruction is attainable.
                    However, the most limiting assumption in these works is that all images of the scene are accurately localized.  
                    To address these challenges, our prior research has advanced techniques for unsupervised 3D pose estimation and 3D reconstruction and manipulation. 
                    </p>
                    <p> Our research goal in this thrust is to build a system capable of holistic 3D scene understanding and reconstruction. 
                    We humans have a holistic understanding of the 3D visual world --- we can easily perceive the object categories, their location, and shapes and even interact with them.
                    This is a fundamental capability required of intelligent agents to navigate and interact with the 3D environment. 
                    Besides this, reconstructing a realistic and immersive virtual 3D world has many applications in VR/AR, robotics, and autonomous driving. 
                    </p>
                    <p>However, such holistic 3D scene understanding and generation are beyond the current state-of-the-art computer vision systems.
                    There are several critical challenges to address.
                    First, scene understanding and 3D reconstruction are usually studied separately, which we believe should be integrated in a way that they are mutually beneficial; 
                    Second, compared to 2D tasks, the lack of human annotations becomes even more problematic for 3D tasks (e.g., 3D object detection and segmentation, 3D pose estimation);
                    Third, unlike 2D images, 3D models are expensive to acquire, especially for deformable objects such as animals;
                    We aim to build a system to holistically understand and reconstruct the 3D scene with minimal human supervision. 
                    <p> <b>[Ongoing projects]</b>  We're developing algorithms to minimize the human annotation cost for open-vocabulary monocular 3D recognition tasks, and we're also building systems for joint semantic and geometry understanding. We are also interested in 3D recontruction and tracking in dynamic scenes.</p>
                    </p>
                  </div>
               </section>

               <section id="cross">
                  <div class="page-header">
                     <h3>Computer Vision + X</h3>
                    <img  src="images/cv_x.png" alt="3D"/>
                    <p> We have sucessfully applied computer vision and machine learning techniques in ecology (e.g., analyzing bird migration using 20 years of extensive radar data) and chemistry (e.g., predicting the hydrocarbon adsorption in zeolites)</p>
                    <p> <b>[Ongoing projects]</b>  We are enthusiastic about building multidisciplinary collaboration and promoting the application of AI across various scientific fields including but not limited to robotics, natual language processing, climate change, and material discovery. Welcome to meet and talk with us about potential cross-field collaboration opportunities!</p>
                  </div>
               </section>

            </div>
         </div>
      </div>

      <!-- Le javascript
         ================================================== -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="js/jquery-1.9.1.min.js"></script>
      <script src="js/bootstrap.min.js"></script>
      <script>
         $(document).ready(function() {
             $(document.body).scrollspy({
                 target: "#navparent"
             });
         });
         
      </script>
   </body>
</html>
